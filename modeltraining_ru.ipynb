{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "cellId": "kr4n52rawbxg209obz4uc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "import re\n",
    "import sys\n",
    "from nltk import NaiveBayesClassifier\n",
    "import nltk.classify\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import pymorphy2\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "cellId": "5v6uejrn9jnnx4z27xryhq"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "cellId": "1guqljko7a74pnp2biyq9m"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.svm import LinearSVC\n",
    "from functools import lru_cache\n",
    "import pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "cellId": "0im96tm4ea6m1o7kevw65hw"
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    return text\n",
    "@lru_cache(100000)\n",
    "def lemma_tokenizer(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    lemmatized_tokens = [morph.parse(token)[0].normal_form for\n",
    "                         token in tokens]\n",
    "    return lemmatized_tokens\n",
    "def ocenkaSex(s):\n",
    "    if s == '+':\n",
    "        return 100\n",
    "    if s == '−':\n",
    "        return 0\n",
    "    return 50\n",
    "def ocenka(s):\n",
    "    if s == '?':\n",
    "        return 0\n",
    "    if s == 'Communication':\n",
    "        return 1\n",
    "    if s == 'Price':\n",
    "        return 2\n",
    "    if s == 'Quality':\n",
    "        return 3\n",
    "    if s == 'Safety':\n",
    "        return 4\n",
    "    assert(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "cellId": "c1a599m7tn6qt61pctnxys"
   },
   "outputs": [],
   "source": [
    "path = 'train.csv'\n",
    "c = pd.read_csv(path, encoding='UTF-8', error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "cellId": "e6m6uot3q55xfdu843ke28"
   },
   "outputs": [],
   "source": [
    "stopWords = stopwords.words('russian')\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "tf_transformer = TfidfTransformer()\n",
    "vectorizer = CountVectorizer(stop_words=stopWords, preprocessor=preprocess_text, tokenizer=lemma_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "cellId": "5amvkwar9u9nzkuiwllq5"
   },
   "outputs": [],
   "source": [
    "stopWords = stopwords.words('russian')\n",
    "stopWords.remove('не')\n",
    "stopWords.remove('нет')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "cellId": "lth1pepcxoldhqm3y64eg"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py:382: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['большой', 'весь', 'всё', 'ещё', 'мочь', 'нибыть', 'свой', 'хороший', 'это'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    }
   ],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()\n",
    "sentences = c['sentence'].tolist()[0:15000]\n",
    "ocenkiSex = list(map(ocenkaSex, c['sentiment'].tolist()))[0:15000]\n",
    "ocenki = list(map(ocenka, c['1category'].tolist()))[0:15000]\n",
    "X = vectorizer.fit_transform(sentences)\n",
    "YSex = np.array(ocenkiSex)\n",
    "Y = np.array(ocenki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "cellId": "nu3rcv5w47ak1x3883wu7"
   },
   "outputs": [],
   "source": [
    "X_train_tf = tf_transformer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "cellId": "zqzmwpakl7p1wiec0qfgm"
   },
   "outputs": [],
   "source": [
    "sents = pd.read_csv('test_for_participants.csv', encoding='UTF-8', error_bad_lines=False)['sentence'].tolist()\n",
    "X_test = tf_transformer.transform(vectorizer.transform(sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "cellId": "8xaglhl9cqmvq0dwjfgp3o"
   },
   "outputs": [],
   "source": [
    "model_RandomForestClassifier = RandomForestClassifier()\n",
    "classifier = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "cellId": "hn8363medwzmnixwgdk6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(X_train_tf, np.array(ocenki))\n",
    "model_RandomForestClassifier.fit(X_train_tf, YSex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "cellId": "0afuudymm1m15qv3ul6x1s"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import math\n",
    "y_pred = classifier.predict(X_test)\n",
    "y_pred_Sex = model_RandomForestClassifier.predict(X_test)\n",
    "y_proba = classifier.predict_proba(X_test)[:, :]\n",
    "y_probaSex = model_RandomForestClassifier.predict_proba(X_test)[:, :]\n",
    "\n",
    "# l = []\n",
    "\n",
    "with open('new.csv', mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    header = ['sentence', '1category', '1categoryprob', '2category', '2categoryprob', 'sentiment', 'sentimentprob']\n",
    "    writer.writerow(header)\n",
    "    for i in range(len(y_pred)):\n",
    "        arr = []\n",
    "        variants = ['?', 'Community', 'Price', 'Quality', 'Safety']\n",
    "        variantsSex = ['-', '?', '+']\n",
    "        classes = classifier.classes_\n",
    "        top_classes = classes[np.argsort(-y_proba)[i][:2]]\n",
    "        classesSex = model_RandomForestClassifier.classes_\n",
    "        top_classesSex = classesSex[np.argsort(-y_probaSex)[i][:2]]\n",
    "        arr.append(sents[i])\n",
    "        arr.append(variants[top_classes[0]])\n",
    "        big = max(y_proba[i][0], y_proba[i][1])\n",
    "        bigSex = max(y_probaSex[i][0], y_probaSex[i][1])\n",
    "        arr.append(f'{big:.2}')\n",
    "        arr.append('')\n",
    "        arr.append('')\n",
    "        if (bigSex < 0.10):\n",
    "            arr.append(variantsSex[classesSex[np.argsort(-y_probaSex)[i][2]]//50])\n",
    "            arr.append(min(y_probaSex[i][0], y_probaSex[i][1], y_probaSex[i][2]))\n",
    "        else:\n",
    "            arr.append(variantsSex[top_classesSex[0]//50])\n",
    "            arr.append(f'{bigSex:.2}')\n",
    "        writer.writerow(arr)\n",
    "        \n",
    "# print(sum(l)/len(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "hpdfeehpi6co7dioeaeog"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "notebookId": "aa70459f-242c-4d52-928d-ec1cbad77bc0",
  "notebookPath": "modeltraining_ru.ipynb",
  "ydsNotebookPath": "modeltraining.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
